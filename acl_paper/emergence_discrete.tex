%
% File acl2020.tex
%
%% Based on the style files for ACL 2020, which were
%% Based on the style files for ACL 2018, NAACL 2018/19, which were
%% Based on the style files for ACL-2015, with some improvements
%%  taken from the NAACL-2016 style
%% Based on the style files for ACL-2014, which were, in turn,
%% based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009,
%% EACL-2009, IJCNLP-2008...
%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{acl2020}
\usepackage{amsmath}
\usepackage{times}
\usepackage{subfigure}
\usepackage{latexsym}
\renewcommand{\UrlFont}{\ttfamily\small}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% TODO(sst): enter aclpaperid here
%\aclfinalcopy % Uncomment this line for the final submission
%\def\aclpaperid{***} %  Enter the acl Paper ID here

%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\title{The spontaneous emergence of discrete and compositional messages}

% TODO(sst): anonymous, so doesn't matter now, but: is this the right author order?
\author{Nur Lan \\
  Computational Linguistics Lab \\
  Tel Aviv University \\
  \texttt{nurlan@mail.tau.ac.il} \\\And
  Shane Steinert-Threlkeld \\
  Department of Linguistics \\
  University of Washington \\
  \texttt{shanest@uw.edu} \\\And
  Emmanuel Chemla \\
  Laboratoire de Sciences Cognitives et Psycholinguistique \\
  Ecole Normale Sup\'erieure \\
  \texttt{chemla@ens.fr}}

\date{}

\usepackage{multicol}  % for big figures
\usepackage{amsmath}
\usepackage{graphicx}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\newcommand{\changeEC}[2]{{\leavevmode\color{gray}{\scriptsize{#1}}~\color{blue}#2}}
\newcommand{\nbEC}[1]{{\leavevmode\color{blue}{\scriptsize#1}}}
\newcommand{\addEC}[1]{{\leavevmode\color{blue}#1}}

\newcommand{\changeNL}[2]{{\leavevmode\color{gray}{\scriptsize{#1}}~\color{red}#2}}
\newcommand{\nbNL}[1]{{\leavevmode\color{red}{\scriptsize#1}}}
\newcommand{\addNL}[1]{{\leavevmode\color{red}#1}}

\newcommand{\changeSST}[2]{{\leavevmode\color{gray}{\scriptsize{#1}}~\color{violet}#2}}
\newcommand{\nbSST}[1]{{\leavevmode\color{violet}{\scriptsize#1}}}
\newcommand{\addSST}[1]{{\leavevmode\color{violet}#1}}


\begin{document}

\maketitle
% TODO(sst): short [4] or long [8] paper?

% TODO(sst): write abstract
\begin{abstract}
	blah blah blah
\end{abstract}

\section{Introduction}

In a signalling game, artificial agents communicate to achieve a common goal: a sender sees some piece of information and produces a message, this message is then sent to a receiver that must take some action \citep{Lewis1969, Skyrms2010}. If the action is appropriate, the whole communication stream, and in particular the choice of the message, is reinforced. For instance, in a referential game, sender and receiver see a set of objects, and the sender must send a message to the receiver, so that the receiver can pick up the right object, as determined in advance for the sender, but unbeknownst to the receiver \citep{Lazaridou2017, Lazaridou2018, Havrylov2017, Chaabouni2019a}.

This setting has been used to study the factors influencing the emergence of various fundamental properties of natural language, such as compositionality \citep{Kirby2015, Franke2016, SteinertThrelkeld2016, Mordatch2018, Lazaridou2018, Choi2018}.

In this paper, we 

\section{Function Games}

We here introduce a general communication game setting, which we call Function Games.  Our games contain three basic components: (i) a set of contexts $C$, (ii) a set of actions $A$, (iii) a family of functions $F$, from contexts to actions.  One play of a Function Game game runs as follows:
\begin{enumerate}
	\item Nature chooses $f \in F$ and a context $c \in C$.
	\item Sender sees the context $c$ and $f$. \nbSST{I like f(c) here, but f is a bit more appropriate.  What do you all think? Nur: Given the situation, f is the only choice, no?}
	\item Sender sends a message $m$ to Receiver.
	\item Receiver sees \emph{a possibly different} context $c'$ and the message $m$ and chooses an action $a'$.
	\item Both are `rewarded' iff $a' = f(c')$.
\end{enumerate}
Two concrete interpretations will be helpful in illustrating the various components.

\noindent \textbf{Generalized referential games.}  A reference game is one in which Sender tries to get Receiver to pick the correct object out of a given set \citep{Skyrms2010, Lazaridou2017, Lazaridou2018, Havrylov2017, Chaabouni2019a}.  Here, contexts are sets of objects (i.e.\ an $m \times n$ matrix, with $m$ objects represented by $n$ features).  Normally (though we will drop this assumption later), $c' = \texttt{shuffled}(c)$: Sender and Receiver see the same objects, but in a different arrangement. Actions are the objects, and the functions $f \in F$ are \emph{choice functions}: $f(c) \in c$ for every context $c$.

\noindent \textbf{Belief update games.}  Contexts can represent possible belief states for the agents.  Letting $A = C$, the functions will then be `belief update' functions, representing e.g.\ how to update an agent's beliefs in the light of learning a new piece of information. \nbSST{What should we cite here? Something from dynamic semantics?}

\section{Experiment}

Because we are interested in the simultaneous emergence both of discrete signals and of compositional messages, we use a Function Game called the Extremity Game designed to incentivize compositionality \citep{Steinert-Threlkeld2019}.  This is a generalized referential game, where objects are represented as $n$-dimensional vectors, with each value corresponding to the degree to which it has a gradable property.  For instance, objects could be shaded circles, with two values, one for their diameter and one for their darkness.  For the functions, we set $F = \left\{ \argmin_i , \argmax_i : 0 \leq i < n \right\}$.  These may incentivize the emergence of communication protocols with messages like `small + EST' and `dark + EST'.
% TODO(sst): cite my forthcoming philosophy of science paper instead? pre-print is only on my website, so URL would be de-anonymizing

\subsection{Model}

Our model resembles an encoder-decoder architecture, with the Sender encoding the context/target pair into a message, and the Receiver decoding the message (together with its context $c'$) into an action.  Both the encoder and decoder are multi-layer perceptrons with two hidden layers of size 64 and rectified linear activation (ReLU) \citep{Nair2010, Glorot2011}. A smaller, intermediate layer without an activation function bridges the encoder and decoder and represents the transformation of the input information to messages. Figure~\ref{fig:model} depicts this architecture.

\begin{figure}[ht]
	\centering
	\includegraphics[width=\columnwidth]{figures/model_figure.png}
	\caption{Model architecture caption \nbSST{Do we really need this? It isn't super informative and might use too much space. I can also do something in TikZ if we think it's important and that this one is ugly.}}
	\label{fig:model}
\end{figure}

\subsection{Game Parameters}

In our experiments, we manipulated the following parameters of the Extremity Game:
\begin{itemize}
	\item Context strictness. 
		In \emph{strict} contexts, every object is the $\argmax$ or $\argmax$ of exactly one dimension.  This means that there is a one-to-one (and onto) correspondence between $F$ and $A = C$.\footnote{These are the contexts used in \cite{Steinert-Threlkeld2019}.}  In \emph{non-strict} contexts, no such restriction is imposed. 

		We considered strict contexts with 10 objects (i.e. each object consists of 5 dimensions) and non-strict contexts with 5, 10, and 15 objects.
	\item Context identity. In the \emph{shared} setting, Receiver sees a shuffled version of Sender's context ($c' = \texttt{shuffled}(c)$). In the \emph{non-shared} setting, Receiver's context $c'$ is entirely distinct from Sender's.  This may incentivize compositional messages, since Sender cannot rely on the raw properties of the target object in communication.
	\item Object size: in all experiments, objects had 5 dimensions.
	\item Latent space (message) dimension: in all experiments, the latent space had 2 dimension\footnote{The model also performs well with messages of size 1, not reported here. Using messages of size 2 makes it easier to inspect the latent space using 2-D visualization.}.
\end{itemize}

\subsection{Training Details}

By using a continuous latent space, the entire model, including the communication channel, is differentiable and so can be trained end-to-end using backpropagation to compute gradients.  We used the Adam optimizer \citep{Kingma2015} with learning rate 0.001, $\beta_1 = 0.9$, and $\beta_2 = 0.999$. The model was trained for 5,000 epochs by feeding the network with mini-batches of 64 contexts concatenated with one-hot function selectors. The network's loss is taken as the MSE between the target object $f(c')$ and the object generated by the Receiver. For each setting of the above parameters, we ran 20 trials with different random seeds.

Code and data will be made available once the paper can be de-anonymized. 

\section{Results}

We measure the communicative success of the network by calculating the accuracy of recovering the correct object from $c'$. The Receiver's prediction is considered correct if its output is closest to $f(c')$ than to all other objects in $c'$. The recovery accuracy of the different settings is reported in Table~\ref{tab:object_prediction_accuracy}. While the network handles well the $c \neq c'$ setting (\textit{unshared context}), the model struggles with non-strict contexts. Note that although accuracy is not $100\%$, it is still well above chance, since e.g. for a context of 10 objects random guessing will yield an expected accuracy of $10\%$. Table~\ref{tab:prediction_trained_untrained} shows the mean loss and recovery accuracy for a network before and after training. The randomly-initialized network's accuracy is at the expected chance level for a 10-object setting.

\begin{table}[]
\begin{tabular}{l|c|c}
\hline
                       & \textbf{Trained} & \textbf{Untrained} \\ \hline
\textbf{\begin{tabular}[c]{@{}l@{}}Accuracy\end{tabular}}  & 63.78$\%\pm 0.02$  & 9.92$\%\pm 0.01$      \\\hline
\textbf{\begin{tabular}[c]{@{}l@{}}Loss (MSE)\end{tabular}}    & $0.04\pm 0.00$  & $0.35\pm 0.03$      \\\hline
\end{tabular}
\caption{Mean object recovery accuracy and prediction loss before and after training, for objects of size 5 in a shared, strict context setting (10 objects per context).}\label{tab:prediction_trained_untrained}
\end{table}


\begin{table}[]
\begin{tabular}{l|c|c}
\hline
                       & \textbf{Shared} & \textbf{Non-shared} \\ \hline
\textbf{\begin{tabular}[c]{@{}l@{}}Strict \\ context \end{tabular}}                 & $63.78\%\pm 1.63$  & $60.22\%\pm 1.56$      \\\hline
\textbf{\begin{tabular}[c]{@{}l@{}}Non-strict, \\ 5 objects\end{tabular}}  & $49.37\%\pm 1.67$  & $43.55\%\pm 1.69$      \\\hline
\textbf{\begin{tabular}[c]{@{}l@{}}Non-strict, \\ 10 objects\end{tabular}} & $33.06\%\pm 1.47$  & $31.89\%\pm 1.63$      \\\hline
\textbf{\begin{tabular}[c]{@{}l@{}}Non-strict, \\ 15 objects\end{tabular}} & $27.58\%\pm 1.30$  & $27.95\%\pm 1.24$      \\ \hline
\end{tabular}
\caption{Object recovery accuracy for the different model settings.}\label{tab:object_prediction_accuracy}
\end{table}


\subsection{Discrete signals}

\begin{figure*}[ht]
  \centering
  Commented out the actual figures because I can't compile without them.
  \subfigure[]{\includegraphics[scale=0.55]{./figures/messages_o5_shared_strict_trained.png}}\hspace{1cm}
  \subfigure[]{\includegraphics[scale=0.55]{./figures/messages_o5_untrained.png}}
  \caption{(a) Messages sampled from latent space of a trained network, for objects of size 5 and contexts of 10 objects. (b) Messages sampled from an untrained network. Colors represent the $f_i \in F$ input part of the Sender.}
 \label{fig:messages}
\end{figure*}

The model's ability to discretizie the communication is measured by inspecting the information generated by the intermediate layer. Figure~\ref{fig:messages} depicts message vectors sampled from this layer. The same is depicted for an untrained network with randomized weights, where the messages are not yet clustered. 

We make the discretization measure concrete by calculating the F1 score between the cluster labels for transmitted messages and the target functions for which they were generated. For this, an unsupervized clustering algorithm is first applied to the message vectors, giving an expected number of clusters (DBSCAN, \citealp{ester_density-based_1996}, with $\epsilon = 0.5$). The cluster labels are then matched with the respective function labels by taking the most common function in each cluster. If message clusters are well separated from one another, the labeling will have less to no confusion and an F1 score closer to~$1$. The F1 scores for the different model settings are given in Table~\ref{tab:f1_scores}. The model reached near-optimal clusterization measures for both shared and non-shared contexts, and for both strict and non-strict contexts. 


\begin{table}[]
\begin{tabular}{l|c|c}
\hline
                       & \textbf{Shared} & \textbf{Non-shared} \\ \hline
\textbf{\begin{tabular}[c]{@{}l@{}}Strict \\ context \end{tabular}}                 & $1.00\pm 0.00$  & $0.90\pm 0.09$      \\\hline
\textbf{\begin{tabular}[c]{@{}l@{}}Non-strict, \\ 5 objects\end{tabular}}  & $0.99\pm 0.02$  & $0.54\pm 0.15$      \\\hline
\textbf{\begin{tabular}[c]{@{}l@{}}Non-strict, \\ 10 objects\end{tabular}} & $1.00\pm 0.00$  & $0.99\pm 0.01$      \\\hline
\textbf{\begin{tabular}[c]{@{}l@{}}Non-strict, \\ 15 objects\end{tabular}} & $1.00\pm 0.00$  & $1.00\pm 0.00$      \\ \hline
\end{tabular}
\caption{Message clusterization F1 scores.}\label{tab:f1_scores}
\end{table}

Given the clusterization of the message space, we are able to sample unseen messages from each cluster, and test the Receiver's perception of `artificial' messages. 10 messages are sampled from each cluster, and their average vector is fed to the Receiver. The output object accuracy for these unseen messages is given in Table~\ref{tab:average_message_accuracy}. The model achieves recovery accuracy similar to when messages are generated using actual inputs. This can be paralleled with the phenomenon of Categorical Perception, which describes how continuous signals, such as phonemes in an acoustic space, are perceived as stable and discrete, even when the signal is gradually shifted in the continuous space.

\begin{table}[]
\begin{tabular}{l|c|c}
\hline
                       & \textbf{Shared} & \textbf{Non-shared} \\ \hline
\textbf{\begin{tabular}[c]{@{}l@{}}Strict \\ context \end{tabular}}                 & $63.39\%\pm 1.45$  & $55.37\%\pm 3.43$      \\\hline
\textbf{\begin{tabular}[c]{@{}l@{}}Non-strict, \\ 5 objects\end{tabular}}  & $46.94\%\pm 1.70$  & $29.40\%\pm 5.59$      \\\hline
\textbf{\begin{tabular}[c]{@{}l@{}}Non-strict, \\ 10 objects\end{tabular}} & $32.63\%\pm 1.43$  & $31.51\%\pm 1.62$      \\\hline
\textbf{\begin{tabular}[c]{@{}l@{}}Non-strict, \\ 15 objects\end{tabular}} & $28.24\%\pm 1.11$  & $27.94\%\pm 1.20$      \\ \hline
\end{tabular}
\caption{Object prediction accuracy using average message from each function cluster. }\label{tab:average_message_accuracy}
\end{table}


\subsection{Compositionality}

We trained a model to predict various features from the message, to see what they encode.  Results show that predicting min/max and param(f) are easy to do.  But this leaves open the question: are these features systematically / compositionally encoded in the message?

First test: analogy; include table

Could be limitation of that method, so try: compositionality network.  Include table here.

\section{Discussion}

\section{Conclusion}

% TODO(sst): add if accepted
% \section*{Acknowledgments}

% The acknowledgments should go immediately before the references. Do not number the acknowledgments section.
% Do not include this section when submitting your paper for review.


\bibliographystyle{acl_natbib}
\bibliography{anthology,acl2020}


\end{document}
