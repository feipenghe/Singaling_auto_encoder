@inproceedings{ester_density-based_1996,
	title = {A density-based algorithm for discovering clusters in large spatial databases with noise.},
	volume = {96},
	booktitle = {Kdd},
	author = {Ester, Martin and Kriegel, Hans-Peter and Sander, JÃ¶rg and Xu, Xiaowei},
	year = {1996},
	pages = {226--231},
	file = {Full Text:/Users/nur/Zotero/storage/29H33CDX/Ester et al. - 1996 - A density-based algorithm for discovering clusters.pdf:application/pdf}
}

@inproceedings{Lazaridou2018,
annote = {* Symbolic input vs. pixel input: why is the latter "more realistic"? And not just a confound on the _linguistic_ part?

* Reference game: sender _only sees target_, receiver has to choose target from a set of distractors
- REINFORCE + entropy regularization

* Symbolic experiment: 573-d binary feature vectors

* Generalization: unseen concepts, unigram chimera, vs. uniform chimera

* 

* Lots more error analysis can be done: what is performance on same-object contexts? what about same-color in the vision case? do they learn to tell them apart, or fail miserably?

* main take away: ability to "scale up traditional research from the language evolution literature"; but is scaling up always a good thing? analyzing what's happening becomes harder, figuring out how to imporve them, etc....},
archivePrefix = {arXiv},
arxivId = {1804.03984v1},
author = {Lazaridou, Angeliki and Hermann, Karl Moritz and Tuyls, Karl and Clark, Stephen},
booktitle = {International Conference of Learning Representations (ICLR 2018)},
eprint = {1804.03984v1},
file = {:Users/shanest/Documents/Library/Lazaridou et al/International Conference of Learning Representations (ICLR 2018)/Lazaridou et al. - 2018 - Emergence of Linguistic Communication from Referential Games with Symbolic and Pixel Input.pdf:pdf},
mendeley-groups = {Philosophy/PhilLang/Semantics and Pragmatics,MachineLearning,Papers/universals,Papers/function-words-psa,Papers/siminf-quant,MachineLearning/emergent-communication,Papers/discrete-acl2020},
title = {{Emergence of Linguistic Communication from Referential Games with Symbolic and Pixel Input}},
year = {2018}
}
@inproceedings{Lazaridou2017,
abstract = {The current mainstream approach to train natural language systems is to expose them to large amounts of text. This passive learning is problematic if we are interested in developing interactive machines, such as conversational agents. We propose a framework for language learning that relies on multi-agent communication. We study this learning in the context of referential games. In these games, a sender and a receiver see a pair of images. The sender is told one of them is the target and is allowed to send a message from a fixed, arbitrary vocabulary to the receiver. The receiver must rely on this message to identify the target. Thus, the agents develop their own language interactively out of the need to communicate. We show that two networks with simple configurations are able to learn to coordinate in the referential game. We further explore how to make changes to the game environment to cause the "word meanings" induced in the game to better reflect intuitive semantic properties of the images. In addition, we present a simple strategy for grounding the agents' code into natural language. Both of these are necessary steps towards developing machines that are able to communicate with humans productively.},
annote = {MAIN SETUP: varied context of 2 images, guess which is intended, atomic symbols only.

1. images i1, ... , iN: nature chooses 2, iL and iR, and one of them to be the target t

2. Sender sees: thetaS(iL, iR, t)

3. Sender sends one symbol s(thetaS(iL, iR, t)) from V

4. Receiver tries to guess target: r(iL, iR, s(thetaS(iL, iR, t))) is L or R

5. If r( ) = t, payoff of 1; payoff of 0 otherwise

RESULTS: learn to communicate; can make results look more "human" by varying images S and R see but in same category},
archivePrefix = {arXiv},
arxivId = {1612.07182},
author = {Lazaridou, Angeliki and Peysakhovich, Alexander and Baroni, Marco},
booktitle = {International Conference of Learning Representations (ICLR2017)},
eprint = {1612.07182},
file = {:Users/shanest/Documents/Library/Lazaridou, Peysakhovich, Baroni/International Conference of Learning Representations (ICLR2017)/Lazaridou, Peysakhovich, Baroni - 2017 - Multi-Agent Cooperation and the Emergence of (Natural) Language.pdf:pdf},
mendeley-groups = {Logic,MachineLearning,Logic/Games,Papers/universals,Papers/function-words-psa,Papers/siminf-quant,MachineLearning/emergent-communication,Papers/discrete-acl2020},
title = {{Multi-Agent Cooperation and the Emergence of (Natural) Language}},
url = {http://arxiv.org/abs/1612.07182},
year = {2017}
}
@inproceedings{Steinert-Threlkeld2019,
abstract = {All natural languages exhibit a distinction between content words (like nouns and adjectives) and function words (like determiners, auxiliaries, prepositions). Yet surprisingly little has been said about the emergence of this universal architectural feature of natural languages. Why have human languages evolved to exhibit this division of labor between content and function words? How could such a distinction have emerged in the first place? This paper takes steps towards answering these questions by showing how the distinction can emerge through reinforcement learning in agents playing a signaling game across contexts which contain multiple objects that possess multiple perceptually salient gradable properties.},
archivePrefix = {arXiv},
arxivId = {1909.11060},
author = {Steinert-Threlkeld, Shane},
booktitle = {Emergent Communication Workshop @ NeurIPS 2018},
eprint = {1909.11060},
file = {:Users/shanest/Documents/Library/Steinert-Threlkeld/Emergent Communication Workshop @ NeurIPS 2018/Steinert-Threlkeld - 2019 - Paying Attention to Function Words.pdf:pdf},
mendeley-groups = {MachineLearning/emergent-communication,Papers/discrete-acl2020},
title = {{Paying Attention to Function Words}},
url = {http://arxiv.org/abs/1909.11060},
year = {2019}
}

@inproceedings{Chaabouni2019a,
archivePrefix = {arXiv},
arxivId = {1905.12561v4},
author = {Chaabouni, Rahma and Kharitonov, Eugene and Dupoux, Emmanuel and Baroni, Marco},
booktitle = {Proceedings of the 33rd Conference on Neural Information Processing Systems (NeurIPS 2019)},
eprint = {1905.12561v4},
file = {:Users/shanest/Documents/Library/Chaabouni et al/Proceedings of the 33rd Conference on Neural Information Processing Systems (NeurIPS 2019)/Chaabouni et al. - 2019 - Anti-efficient encoding in emergent communication.pdf:pdf;:Users/shanest/Documents/Library/Chaabouni et al/Proceedings of the 33rd Conference on Neural Information Processing Systems (NeurIPS 2019)/Chaabouni et al. - 2019 - Anti-efficient encoding in emergent communication(2).pdf:pdf},
mendeley-groups = {MachineLearning,_reading-list,MachineLearning/emergent-communication,Papers/discrete-acl2020},
title = {{Anti-efficient encoding in emergent communication}},
year = {2019}
}

@inproceedings{Havrylov2017,
abstract = {Learning to communicate through interaction, rather than relying on explicit supervision, is often considered a prerequisite for developing a general AI. We study a setting where two agents engage in playing a referential game and, from scratch, develop a communication protocol necessary to succeed in this game. Unlike previous work, we require that messages they exchange, both at train and test time, are in the form of a language (i.e. sequences of discrete symbols). We compare a reinforcement learning approach and one using a differentiable relaxation (straight-through Gumbel-softmax estimator) and observe that the latter is much faster to converge and it results in more effective protocols. Interestingly, we also observe that the protocol we induce by optimizing the communication success exhibits a degree of compositionality and variability (i.e. the same information can be phrased in different ways), both properties characteristic of natural languages. As the ultimate goal is to ensure that communication is accomplished in natural language, we also perform experiments where we inject prior information about natural language into our model and study properties of the resulting protocol.},
annote = {MAIN SETUP: context varies = subset of images and a target; sender only sees target, but sends _sequence_

1. images i1, ..., iN; target t, distractors d1, ..., dk

2. sender sends Sphi(t) is sequence of symbols from V of length up to L

3. receiver: Rtheta(Sphi(t), d1, ..., dk) guesses t

MAIN RESULT: sequences are "hierarchical": animal --> bear; food -> baked -> pizza

Other contributions: relaxations using straight-through estimators better than reinforcement learning},
archivePrefix = {arXiv},
arxivId = {1705.11192},
author = {Havrylov, Serhii and Titov, Ivan},
booktitle = {Proceedings of the 31st Conference on Neural Information Processing Systems (NeurIPS 2017)},
eprint = {1705.11192},
file = {:Users/shanest/Documents/Library/Havrylov, Titov/Proceedings of the 31st Conference on Neural Information Processing Systems (NeurIPS 2017)/Havrylov, Titov - 2017 - Emergence of Language with Multi-agent Games Learning to Communicate with Sequences of Symbols.pdf:pdf},
mendeley-groups = {MachineLearning,Logic/Games,Papers/universals,Papers/function-words-psa,Papers/siminf-quant,MachineLearning/emergent-communication,Papers/discrete-acl2020},
title = {{Emergence of Language with Multi-agent Games: Learning to Communicate with Sequences of Symbols}},
url = {http://arxiv.org/abs/1705.11192},
year = {2017}
}

@book{Skyrms2010,
author = {Skyrms, Brian},
keywords = {Meaning},
mendeley-groups = {Logic/Games,Papers/MonkeyCalls,Papers/JoLLI_IDAS,Papers/LING236,Zotero - Zotero Library,Dissertation,Papers/function-words-psa,Papers/siminf-quant,Papers/discrete-acl2020},
mendeley-tags = {Meaning},
publisher = {Oxford University Press},
shorttitle = {Signals},
title = {{Signals: Evolution, Learning, and Information}},
year = {2010}
}

@inproceedings{Glorot2011,
abstract = {While logistic sigmoid neurons are more biologically plausable that hyperbolic tangent neurons, the latter work better for training multi-layer neural networks. This paper shows that rectifying neurons are an even better model of biological neurons and yield equal or better performance than hyperbolic tangent networks in spite of the hard non-linearity and non-differentiability at zero, creating sparse representations with true zeros, which seem remarkably suitable for naturally sparse data. Even though they can take advantage of semi-supervised setups with extra-unlabelled data, deep rectifier networks can reach their best performance without requiring any unsupervised pre-training on purely supervised tasks with large labelled data sets. Hence, these results can be seen as a new milestone in the attempts at understanding the difficulty in training deep but purely supervised nueral networks, and closing the performance gap between neural networks learnt with and without unsupervised pre-training},
author = {Glorot, Xavier and Bordes, Antoine and Bengio, Yoshua},
booktitle = {14th International Conference on Artificial Intelligence and Statistics (AISTATS)},
file = {:Users/shanest/Documents/Library/Glorot, Bordes, Bengio/14th International Conference on Artificial Intelligence and Statistics (AISTATS)/Glorot, Bordes, Bengio - 2011 - Deep Sparse Rectifier Neural Networks.pdf:pdf},
mendeley-groups = {MachineLearning,Papers/discrete-acl2020},
pages = {315--323},
title = {{Deep Sparse Rectifier Neural Networks}},
year = {2011}
}

@inproceedings{Kingma2015,
archivePrefix = {arXiv},
arxivId = {1412.6980},
author = {Kingma, Diederik P. and Ba, Jimmy},
booktitle = {International {Conference} of {Learning} {Representations} ({ICLR})},
eprint = {1412.6980},
file = {:Users/shanest/Documents/Library/Kingma, Ba/International {Conference} of {Learning} {Representations} ({ICLR})/Kingma, Ba - 2015 - Adam A Method for Stochastic Optimization.pdf:pdf},
mendeley-groups = {Papers/quant-rnn,MachineLearning,Papers/universals,Papers/logic-cognition-lecture,Papers/function-words-psa,Papers/neural-most,Papers/siminf-quant,Papers/discrete-acl2020},
title = {{Adam: A Method for Stochastic Optimization}},
url = {https://arxiv.org/abs/1412.6980},
year = {2015}
}

@inproceedings{Nair2010,
author = {Nair, Vinod and Hinton, Geoffrey E},
booktitle = {Proceedings of the 27th International Conference on Machine Learning (ICML)},
file = {:Users/shanest/Documents/Library/Nair, Hinton/Proceedings of the 27th International Conference on Machine Learning (ICML)/Nair, Hinton - 2010 - Rectified Linear Units Improve Restricted Boltzmann Machines.pdf:pdf},
mendeley-groups = {MachineLearning,Papers/discrete-acl2020},
title = {{Rectified Linear Units Improve Restricted Boltzmann Machines}},
url = {https://www.semanticscholar.org/paper/Rectified-Linear-Units-Improve-Restricted-Boltzmann-Nair-Hinton/a538b05ebb01a40323997629e171c91aa28b8e2f},
year = {2010}
}

