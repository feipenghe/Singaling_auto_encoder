%% This BibTeX bibliography file was created using BibDesk.
%% http://bibdesk.sourceforge.net/

%% Created for Emmanuel Chemla at 2019-12-09 13:12:27 +0100 


%% Saved with string encoding Unicode (UTF-8) 


@article{mikolov_efficient_2013,
	title = {Efficient {Estimation} of {Word} {Representations} in {Vector} {Space}},
	url = {http://arxiv.org/abs/1301.3781},
	abstract = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
	urldate = {2019-12-09},
	journal = {arXiv:1301.3781 [cs]},
	author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
	month = sep,
	year = {2013},
	note = {arXiv: 1301.3781},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/nur/Zotero/storage/87RM6P85/Mikolov et al. - 2013 - Efficient Estimation of Word Representations in Ve.pdf:application/pdf;arXiv.org Snapshot:/Users/nur/Zotero/storage/P75KFKWQ/1301.html:text/html}
}

@misc{DamperHarnad-CP,
	Abstract = {Studies of the categorical perception (CP) of sensory continua have a long and rich history in
psychophysics. In 1977, Macmillan et al. introduced the use of signal detection theory to CP
studies. Anderson et al. simultaneously proposed the first neural model for CP, yet this line
of research has been less well explored. In this paper, we assess the ability of neural-network
models of CP to predict the psychophysical performance of real observers with speech sounds
and artificial/novel stimuli. We show that a variety of neural mechanisms is capable of gen-erating
the characteristics of categorical perception. Hence, CP may not be a special mode of
perception but an emergent property of any sufficiently powerful general learning system.},
	Author = {R.I. Damper and S.R. Harnad},
	Date-Added = {2019-12-09 13:12:15 +0100},
	Date-Modified = {2019-12-09 13:12:27 +0100},
	Journal = {Perception and Psychophysics},
	Keywords = {categorical perception, neural networks},
	Number = {4},
	Pages = {843--867},
	Title = {Neural Network Models of Categorical Perception},
	Url = {http://cogprints.org/1620/},
	Volume = {62},
	Year = {2000},
	Bdsk-Url-1 = {http://cogprints.org/1620/}}

@inproceedings{ester_density-based_1996,
	Author = {Ester, Martin and Kriegel, Hans-Peter and Sander, J{\"o}rg and Xu, Xiaowei},
	Booktitle = {Kdd},
	File = {Full Text:/Users/nur/Zotero/storage/29H33CDX/Ester et al. - 1996 - A density-based algorithm for discovering clusters.pdf:application/pdf},
	Pages = {226--231},
	Title = {A density-based algorithm for discovering clusters in large spatial databases with noise.},
	Volume = {96},
	Year = {1996}}

@inproceedings{Lazaridou2018,
	Annote = {* Symbolic input vs. pixel input: why is the latter "more realistic"? And not just a confound on the _linguistic_ part?

* Reference game: sender _only sees target_, receiver has to choose target from a set of distractors
- REINFORCE + entropy regularization

* Symbolic experiment: 573-d binary feature vectors

* Generalization: unseen concepts, unigram chimera, vs. uniform chimera

* 

* Lots more error analysis can be done: what is performance on same-object contexts? what about same-color in the vision case? do they learn to tell them apart, or fail miserably?

* main take away: ability to "scale up traditional research from the language evolution literature"; but is scaling up always a good thing? analyzing what's happening becomes harder, figuring out how to imporve them, etc....},
	Archiveprefix = {arXiv},
	Arxivid = {1804.03984v1},
	Author = {Lazaridou, Angeliki and Hermann, Karl Moritz and Tuyls, Karl and Clark, Stephen},
	Booktitle = {International Conference of Learning Representations (ICLR 2018)},
	Eprint = {1804.03984v1},
	File = {:Users/shanest/Documents/Library/Lazaridou et al/International Conference of Learning Representations (ICLR 2018)/Lazaridou et al. - 2018 - Emergence of Linguistic Communication from Referential Games with Symbolic and Pixel Input.pdf:pdf},
	Mendeley-Groups = {Philosophy/PhilLang/Semantics and Pragmatics,MachineLearning,Papers/universals,Papers/function-words-psa,Papers/siminf-quant,MachineLearning/emergent-communication,Papers/discrete-acl2020},
	Title = {{Emergence of Linguistic Communication from Referential Games with Symbolic and Pixel Input}},
	Year = {2018}}

@inproceedings{Lazaridou2017,
	Abstract = {The current mainstream approach to train natural language systems is to expose them to large amounts of text. This passive learning is problematic if we are interested in developing interactive machines, such as conversational agents. We propose a framework for language learning that relies on multi-agent communication. We study this learning in the context of referential games. In these games, a sender and a receiver see a pair of images. The sender is told one of them is the target and is allowed to send a message from a fixed, arbitrary vocabulary to the receiver. The receiver must rely on this message to identify the target. Thus, the agents develop their own language interactively out of the need to communicate. We show that two networks with simple configurations are able to learn to coordinate in the referential game. We further explore how to make changes to the game environment to cause the "word meanings" induced in the game to better reflect intuitive semantic properties of the images. In addition, we present a simple strategy for grounding the agents' code into natural language. Both of these are necessary steps towards developing machines that are able to communicate with humans productively.},
	Annote = {MAIN SETUP: varied context of 2 images, guess which is intended, atomic symbols only.

1. images i1, ... , iN: nature chooses 2, iL and iR, and one of them to be the target t

2. Sender sees: thetaS(iL, iR, t)

3. Sender sends one symbol s(thetaS(iL, iR, t)) from V

4. Receiver tries to guess target: r(iL, iR, s(thetaS(iL, iR, t))) is L or R

5. If r( ) = t, payoff of 1; payoff of 0 otherwise

RESULTS: learn to communicate; can make results look more "human" by varying images S and R see but in same category},
	Archiveprefix = {arXiv},
	Arxivid = {1612.07182},
	Author = {Lazaridou, Angeliki and Peysakhovich, Alexander and Baroni, Marco},
	Booktitle = {International Conference of Learning Representations (ICLR2017)},
	Eprint = {1612.07182},
	File = {:Users/shanest/Documents/Library/Lazaridou, Peysakhovich, Baroni/International Conference of Learning Representations (ICLR2017)/Lazaridou, Peysakhovich, Baroni - 2017 - Multi-Agent Cooperation and the Emergence of (Natural) Language.pdf:pdf},
	Mendeley-Groups = {Logic,MachineLearning,Logic/Games,Papers/universals,Papers/function-words-psa,Papers/siminf-quant,MachineLearning/emergent-communication,Papers/discrete-acl2020},
	Title = {{Multi-Agent Cooperation and the Emergence of (Natural) Language}},
	Url = {http://arxiv.org/abs/1612.07182},
	Year = {2017},
	Bdsk-Url-1 = {http://arxiv.org/abs/1612.07182}}

@inproceedings{Steinert-Threlkeld2019,
	Abstract = {All natural languages exhibit a distinction between content words (like nouns and adjectives) and function words (like determiners, auxiliaries, prepositions). Yet surprisingly little has been said about the emergence of this universal architectural feature of natural languages. Why have human languages evolved to exhibit this division of labor between content and function words? How could such a distinction have emerged in the first place? This paper takes steps towards answering these questions by showing how the distinction can emerge through reinforcement learning in agents playing a signaling game across contexts which contain multiple objects that possess multiple perceptually salient gradable properties.},
	Archiveprefix = {arXiv},
	Arxivid = {1909.11060},
	Author = {Steinert-Threlkeld, Shane},
	Booktitle = {Emergent Communication Workshop @ NeurIPS 2018},
	Eprint = {1909.11060},
	File = {:Users/shanest/Documents/Library/Steinert-Threlkeld/Emergent Communication Workshop @ NeurIPS 2018/Steinert-Threlkeld - 2019 - Paying Attention to Function Words.pdf:pdf},
	Mendeley-Groups = {MachineLearning/emergent-communication,Papers/discrete-acl2020},
	Title = {{Paying Attention to Function Words}},
	Url = {http://arxiv.org/abs/1909.11060},
	Year = {2019},
	Bdsk-Url-1 = {http://arxiv.org/abs/1909.11060}}

@inproceedings{Chaabouni2019a,
	Archiveprefix = {arXiv},
	Arxivid = {1905.12561v4},
	Author = {Chaabouni, Rahma and Kharitonov, Eugene and Dupoux, Emmanuel and Baroni, Marco},
	Booktitle = {Proceedings of the 33rd Conference on Neural Information Processing Systems (NeurIPS 2019)},
	Eprint = {1905.12561v4},
	File = {:Users/shanest/Documents/Library/Chaabouni et al/Proceedings of the 33rd Conference on Neural Information Processing Systems (NeurIPS 2019)/Chaabouni et al. - 2019 - Anti-efficient encoding in emergent communication.pdf:pdf;:Users/shanest/Documents/Library/Chaabouni et al/Proceedings of the 33rd Conference on Neural Information Processing Systems (NeurIPS 2019)/Chaabouni et al. - 2019 - Anti-efficient encoding in emergent communication(2).pdf:pdf},
	Mendeley-Groups = {MachineLearning,_reading-list,MachineLearning/emergent-communication,Papers/discrete-acl2020},
	Title = {{Anti-efficient encoding in emergent communication}},
	Year = {2019}}

@inproceedings{Havrylov2017,
	Abstract = {Learning to communicate through interaction, rather than relying on explicit supervision, is often considered a prerequisite for developing a general AI. We study a setting where two agents engage in playing a referential game and, from scratch, develop a communication protocol necessary to succeed in this game. Unlike previous work, we require that messages they exchange, both at train and test time, are in the form of a language (i.e. sequences of discrete symbols). We compare a reinforcement learning approach and one using a differentiable relaxation (straight-through Gumbel-softmax estimator) and observe that the latter is much faster to converge and it results in more effective protocols. Interestingly, we also observe that the protocol we induce by optimizing the communication success exhibits a degree of compositionality and variability (i.e. the same information can be phrased in different ways), both properties characteristic of natural languages. As the ultimate goal is to ensure that communication is accomplished in natural language, we also perform experiments where we inject prior information about natural language into our model and study properties of the resulting protocol.},
	Annote = {MAIN SETUP: context varies = subset of images and a target; sender only sees target, but sends _sequence_

1. images i1, ..., iN; target t, distractors d1, ..., dk

2. sender sends Sphi(t) is sequence of symbols from V of length up to L

3. receiver: Rtheta(Sphi(t), d1, ..., dk) guesses t

MAIN RESULT: sequences are "hierarchical": animal --> bear; food -> baked -> pizza

Other contributions: relaxations using straight-through estimators better than reinforcement learning},
	Archiveprefix = {arXiv},
	Arxivid = {1705.11192},
	Author = {Havrylov, Serhii and Titov, Ivan},
	Booktitle = {Proceedings of the 31st Conference on Neural Information Processing Systems (NeurIPS 2017)},
	Eprint = {1705.11192},
	File = {:Users/shanest/Documents/Library/Havrylov, Titov/Proceedings of the 31st Conference on Neural Information Processing Systems (NeurIPS 2017)/Havrylov, Titov - 2017 - Emergence of Language with Multi-agent Games Learning to Communicate with Sequences of Symbols.pdf:pdf},
	Mendeley-Groups = {MachineLearning,Logic/Games,Papers/universals,Papers/function-words-psa,Papers/siminf-quant,MachineLearning/emergent-communication,Papers/discrete-acl2020},
	Title = {{Emergence of Language with Multi-agent Games: Learning to Communicate with Sequences of Symbols}},
	Url = {http://arxiv.org/abs/1705.11192},
	Year = {2017},
	Bdsk-Url-1 = {http://arxiv.org/abs/1705.11192}}

@book{Skyrms2010,
	Author = {Skyrms, Brian},
	Keywords = {Meaning},
	Mendeley-Groups = {Logic/Games,Papers/MonkeyCalls,Papers/JoLLI_IDAS,Papers/LING236,Zotero - Zotero Library,Dissertation,Papers/function-words-psa,Papers/siminf-quant,Papers/discrete-acl2020},
	Mendeley-Tags = {Meaning},
	Publisher = {Oxford University Press},
	Shorttitle = {Signals},
	Title = {{Signals: Evolution, Learning, and Information}},
	Year = {2010}}

@inproceedings{Glorot2011,
	Abstract = {While logistic sigmoid neurons are more biologically plausable that hyperbolic tangent neurons, the latter work better for training multi-layer neural networks. This paper shows that rectifying neurons are an even better model of biological neurons and yield equal or better performance than hyperbolic tangent networks in spite of the hard non-linearity and non-differentiability at zero, creating sparse representations with true zeros, which seem remarkably suitable for naturally sparse data. Even though they can take advantage of semi-supervised setups with extra-unlabelled data, deep rectifier networks can reach their best performance without requiring any unsupervised pre-training on purely supervised tasks with large labelled data sets. Hence, these results can be seen as a new milestone in the attempts at understanding the difficulty in training deep but purely supervised nueral networks, and closing the performance gap between neural networks learnt with and without unsupervised pre-training},
	Author = {Glorot, Xavier and Bordes, Antoine and Bengio, Yoshua},
	Booktitle = {14th International Conference on Artificial Intelligence and Statistics (AISTATS)},
	File = {:Users/shanest/Documents/Library/Glorot, Bordes, Bengio/14th International Conference on Artificial Intelligence and Statistics (AISTATS)/Glorot, Bordes, Bengio - 2011 - Deep Sparse Rectifier Neural Networks.pdf:pdf},
	Mendeley-Groups = {MachineLearning,Papers/discrete-acl2020},
	Pages = {315--323},
	Title = {{Deep Sparse Rectifier Neural Networks}},
	Year = {2011}}

@inproceedings{Kingma2015,
	Archiveprefix = {arXiv},
	Arxivid = {1412.6980},
	Author = {Kingma, Diederik P. and Ba, Jimmy},
	Booktitle = {International {Conference} of {Learning} {Representations} ({ICLR})},
	Eprint = {1412.6980},
	File = {:Users/shanest/Documents/Library/Kingma, Ba/International {Conference} of {Learning} {Representations} ({ICLR})/Kingma, Ba - 2015 - Adam A Method for Stochastic Optimization.pdf:pdf},
	Mendeley-Groups = {Papers/quant-rnn,MachineLearning,Papers/universals,Papers/logic-cognition-lecture,Papers/function-words-psa,Papers/neural-most,Papers/siminf-quant,Papers/discrete-acl2020},
	Title = {{Adam: A Method for Stochastic Optimization}},
	Url = {https://arxiv.org/abs/1412.6980},
	Year = {2015},
	Bdsk-Url-1 = {https://arxiv.org/abs/1412.6980}}

@inproceedings{Nair2010,
	Author = {Nair, Vinod and Hinton, Geoffrey E},
	Booktitle = {Proceedings of the 27th International Conference on Machine Learning (ICML)},
	File = {:Users/shanest/Documents/Library/Nair, Hinton/Proceedings of the 27th International Conference on Machine Learning (ICML)/Nair, Hinton - 2010 - Rectified Linear Units Improve Restricted Boltzmann Machines.pdf:pdf},
	Mendeley-Groups = {MachineLearning,Papers/discrete-acl2020},
	Title = {{Rectified Linear Units Improve Restricted Boltzmann Machines}},
	Url = {https://www.semanticscholar.org/paper/Rectified-Linear-Units-Improve-Restricted-Boltzmann-Nair-Hinton/a538b05ebb01a40323997629e171c91aa28b8e2f},
	Year = {2010},
	Bdsk-Url-1 = {https://www.semanticscholar.org/paper/Rectified-Linear-Units-Improve-Restricted-Boltzmann-Nair-Hinton/a538b05ebb01a40323997629e171c91aa28b8e2f}}

@article{Hockett1960,
	Author = {Hockett, Charles F},
	File = {:Users/shanest/Documents/Library/Hockett/Scienctific American/Hockett - 1960 - The Origin of Speech.pdf:pdf},
	Journal = {Scienctific American},
	Mendeley-Groups = {MachineLearning/emergent-communication},
	Pages = {88--111},
	Title = {{The Origin of Speech}},
	Volume = {203},
	Year = {1960}}

@book{Lewis1969,
	Author = {Lewis, David},
	Mendeley-Groups = {Logic/Games,Papers/MonkeyCalls,Papers/JoLLI_IDAS,Philosophy/PhilLang/Semantics and Pragmatics,Papers/LING236,Philosophy/PhilLang,Dissertation,Papers/function-words-psa},
	Publisher = {Blackwell},
	Title = {{Convention}},
	Year = {1969}}

@inproceedings{Mordatch2018,
	Abstract = {By capturing statistical patterns in large corpora, machine learning has enabled significant advances in natural language processing, including in machine translation, question answering, and sentiment analysis. However, for agents to intelligently interact with humans, simply capturing the statistical patterns is insufficient. In this paper we investigate if, and how, grounded compositional language can emerge as a means to achieve goals in multi-agent populations. Towards this end, we propose a multi-agent learning environment and learning methods that bring about emergence of a basic compositional language. This language is represented as streams of abstract discrete symbols uttered by agents over time, but nonetheless has a coherent structure that possesses a defined vocabulary and syntax. We also observe emergence of non-verbal communication such as pointing and guiding when language communication is unavailable.},
	Author = {Mordatch, Igor and Abbeel, Pieter},
	Booktitle = {The Thirty-Second AAAI Conference on Artificial Intelligence (AAAI 2018)},
	File = {:Users/shanest/Documents/Library/Mordatch, Abbeel/The Thirty-Second AAAI Conference on Artificial Intelligence (AAAI 2018)/Mordatch, Abbeel - 2018 - Emergence of Grounded Compositional Language in Multi-Agent Populations.pdf:pdf},
	Mendeley-Groups = {Logic,MachineLearning,Logic/Games,Papers/universals,Papers/function-words-psa,Papers/siminf-quant,MachineLearning/emergent-communication},
	Title = {{Emergence of Grounded Compositional Language in Multi-Agent Populations}},
	Url = {http://arxiv.org/abs/1703.04908},
	Year = {2018},
	Bdsk-Url-1 = {http://arxiv.org/abs/1703.04908}}

@inproceedings{Choi2018,
	Annote = {* Images of a single object: 8 shapes, 5 colors = 40 combinations
- 100 from each cell, like my idea w/ M Franke

* Sender / Receiver each get an image
- communication goal: is R seeing same object (out of 40) as S?

* Obverter: Sender chooses next symbol in seq based on which maximizes the probability it would assign to the object it's seeing

* "Grammar Analysis": seems very far-fetched to say robust structure / compositional; the meanings assigned to the "decomposed" signals are _very_ disjunctive

* ZERO SHOT EXPERIMENT:
- train only on 35/40 combos, test generalization to the rest
- just like M Franke and I were discussing},
	Archiveprefix = {arXiv},
	Arxivid = {1804.02341v1},
	Author = {Choi, Edward and Lazaridou, Angeliki and de Freitas, Nando},
	Booktitle = {International Conference of Learning Representations (ICLR 2018)},
	Eprint = {1804.02341v1},
	File = {:Users/shanest/Documents/Library/Choi, Lazaridou, Freitas/International Conference of Learning Representations (ICLR 2018)/Choi, Lazaridou, Freitas - 2018 - Compositional Obverter Communication Learning from Raw Visual Input.pdf:pdf},
	Mendeley-Groups = {MachineLearning,MachineLearning/emergent-communication},
	Pages = {1--18},
	Title = {{Compositional Obverter Communication Learning from Raw Visual Input}},
	Year = {2018}}

@article{Kirby2015,
	Author = {Kirby, Simon and Tamariz, Monica and Cornish, Hannah and Smith, Kenny},
	Doi = {10.1016/j.cognition.2015.03.016},
	File = {:Users/shanest/Documents/Library/Kirby et al/Cognition/Kirby et al. - 2015 - Compression and communication in the cultural evolution of linguistic structure.pdf:pdf},
	Issn = {00100277},
	Journal = {Cognition},
	Keywords = {cultural transmission},
	Mendeley-Groups = {Philosophy/CognitiveScience,Dissertation},
	Pages = {87--102},
	Publisher = {Elsevier B.V.},
	Title = {{Compression and communication in the cultural evolution of linguistic structure}},
	Url = {http://linkinghub.elsevier.com/retrieve/pii/S0010027715000815},
	Volume = {141},
	Year = {2015},
	Bdsk-Url-1 = {http://linkinghub.elsevier.com/retrieve/pii/S0010027715000815},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.cognition.2015.03.016}}

@article{Franke2016,
	Abstract = {Compositionality is a key design feature of human language: the meaning of complex expressions is, for the most part, systematically constructed from the meanings of its parts and their manner of composition. This paper demonstrates that rudimentary forms of compositional communicative behavior can emerge from a variant of reinforcement learning applied to signaling games. This helps explain how compositionality could have emerged gradually: if unsophisticated agents can evolve prevalent dispositions to communicate compositional-like, there is a direct evolution- ary benefit for adaptations that exploit the systematicity in form-meaning mappings more rigorously.},
	Author = {Franke, Michael},
	Doi = {10.1007/s10849-015-9232-5},
	File = {:Users/shanest/Documents/Library/Franke/Journal of Logic, Language and Information/Franke - 2016 - The Evolution of Compositionality in Signaling Games.pdf:pdf},
	Journal = {Journal of Logic, Language and Information},
	Keywords = {Compositionality,Reinforcement learning,Signaling games},
	Mendeley-Groups = {Logic/Games,Philosophy/PhilLang/Semantics and Pragmatics,Dissertation,Papers/function-words-psa,Papers},
	Title = {{The Evolution of Compositionality in Signaling Games}},
	Year = {2016},
	Bdsk-Url-1 = {https://doi.org/10.1007/s10849-015-9232-5}}

@article{SteinertThrelkeld2016,
	Author = {Steinert-Threlkeld, Shane},
	Doi = {10.1007/s10849-016-9236-9},
	File = {:Users/shanest/Documents/Library/Steinert-Threlkeld/Journal of Logic, Language and Information/Steinert-Threlkeld - 2016 - Compositional Signaling in a Complex World.pdf:pdf},
	Journal = {Journal of Logic, Language and Information},
	Mendeley-Groups = {Papers/JoLLI_IDAS,Papers/LING236,Dissertation,Papers/BSD16comments,Papers/universals,Papers/function-words-psa,Papers/siminf-quant},
	Number = {3},
	Pages = {379--397},
	Title = {{Compositional Signaling in a Complex World}},
	Volume = {25},
	Year = {2016},
	Bdsk-Url-1 = {https://doi.org/10.1007/s10849-016-9236-9}}

@article{Williams1992,
	Author = {Williams, Ronald J},
	Journal = {Machine Learning},
	Mendeley-Groups = {MachineLearning,Papers/function-words-psa,Papers/neural-most},
	Number = {3-4},
	Pages = {229--256},
	Title = {{Simple statistical gradient-following algorithms for connectionist reinforcement learning}},
	Volume = {8},
	Year = {1992}}

@inproceedings{Mnih2016,
	Abstract = {We propose a conceptually simple and lightweight framework for deep reinforce- ment learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task of navigating random 3D mazes using a visual input.},
	Author = {Mnih, Volodymyr and {Puigdom{\`{e}}nech Badia}, Adri{\`{a}} and Mirza, Mehdi and Harley, Tim and Lillicrap, Timothy P and Silver, David and Kavukcuoglu, Koray},
	Booktitle = {International Conference on Machine Learning (ICML)},
	File = {:Users/shanest/Documents/Library/Mnih et al/International Conference on Machine Learning (ICML)/Mnih et al. - 2016 - Asynchronous Methods for Deep Reinforcement Learning.pdf:pdf},
	Mendeley-Groups = {MachineLearning},
	Title = {{Asynchronous Methods for Deep Reinforcement Learning}},
	Url = {http://proceedings.mlr.press/v48/mniha16.html},
	Year = {2016},
	Bdsk-Url-1 = {http://proceedings.mlr.press/v48/mniha16.html}}

@inproceedings{Schulman2015,
	Abstract = {In a variety of problems originating in supervised, unsupervised, and reinforcement learning, the loss function is defined by an expectation over a collection of random variables, which might be part of a probabilistic model or the external world. Estimating the gradient of this loss function, using samples, lies at the core of gradient-based learning algorithms for these problems. We introduce the formalism of stochastic computation graphs---directed acyclic graphs that include both deterministic functions and conditional probability distributions---and describe how to easily and automatically derive an unbiased estimator of the loss function's gradient. The resulting algorithm for computing the gradient estimator is a simple modification of the standard backpropagation algorithm. The generic scheme we propose unifies estimators derived in variety of prior work, along with variance-reduction techniques therein. It could assist researchers in developing intricate models involving a combination of stochastic and deterministic operations, enabling, for example, attention, memory, and control actions.},
	Archiveprefix = {arXiv},
	Arxivid = {1506.05254},
	Author = {Schulman, John and Heess, Nicolas and Weber, Theophane and Abbeel, Pieter},
	Booktitle = {Advances in Neural Information Processing Systems 28 (NIPS 2015)},
	Eprint = {1506.05254},
	File = {:Users/shanest/Documents/Library/Schulman et al/Advances in Neural Information Processing Systems 28 (NIPS 2015)/Schulman et al. - 2015 - Gradient Estimation Using Stochastic Computation Graphs.pdf:pdf},
	Mendeley-Groups = {MachineLearning},
	Title = {{Gradient Estimation Using Stochastic Computation Graphs}},
	Url = {http://arxiv.org/abs/1506.05254},
	Year = {2015},
	Bdsk-Url-1 = {http://arxiv.org/abs/1506.05254}}

@inproceedings{Kharitonov2019,
	Abstract = {There is renewed interest in simulating language emergence among deep neural agents that communicate to jointly solve a task, spurred by the practical aim to develop language-enabled interactive AIs, as well as by theoretical questions about the evolution of human language. However, optimizing deep architectures connected by a discrete communication channel (such as that in which language emerges) is technically challenging. We introduce EGG, a toolkit that greatly simplifies the implementation of emergent-language communication games. EGG's modular design provides a set of building blocks that the user can combine to create new games, easily navigating the optimization and architecture space. We hope that the tool will lower the technical barrier, and encourage researchers from various backgrounds to do original work in this exciting area.},
	Address = {Stroudsburg, PA, USA},
	Archiveprefix = {arXiv},
	Arxivid = {1907.00852},
	Author = {Kharitonov, Eugene and Chaabouni, Rahma and Bouchacourt, Diane and Baroni, Marco},
	Booktitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP): System Demonstrations},
	Doi = {10.18653/v1/D19-3010},
	Eprint = {1907.00852},
	File = {:Users/shanest/Documents/Library/Kharitonov et al/Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural./Kharitonov et al. - 2019 - EGG a toolkit for research on Emergence of lanGuage in Games.pdf:pdf},
	Mendeley-Groups = {MachineLearning/emergent-communication},
	Pages = {55--60},
	Publisher = {Association for Computational Linguistics},
	Title = {{EGG: a toolkit for research on Emergence of lanGuage in Games}},
	Url = {https://www.aclweb.org/anthology/D19-3010},
	Year = {2019},
	Bdsk-Url-1 = {https://www.aclweb.org/anthology/D19-3010},
	Bdsk-Url-2 = {https://doi.org/10.18653/v1/D19-3010}}

@inproceedings{Maddison2017,
	Abstract = {The reparameterization trick enables optimizing large scale stochastic computation graphs via gradient descent. The essence of the trick is to refactor each stochastic node into a differentiable function of its parameters and a random variable with fixed distribution. After refactoring, the gradients of the loss propagated by the chain rule through the graph are low variance unbiased estimators of the gradients of the expected loss. While many continuous random variables have such reparameterizations, discrete random variables lack useful reparameterizations due to the discontinuous nature of discrete states. In this work we introduce CONCRETE random variables---CONtinuous relaxations of disCRETE random variables. The Concrete distribution is a new family of distributions with closed form densities and a simple reparameterization. Whenever a discrete stochastic node of a computation graph can be refactored into a one-hot bit representation that is treated continuously, Concrete stochastic nodes can be used with automatic differentiation to produce low-variance biased gradients of objectives (including objectives that depend on the log-probability of latent stochastic nodes) on the corresponding discrete graph. We demonstrate the effectiveness of Concrete relaxations on density estimation and structured prediction tasks using neural networks.},
	Author = {Maddison, Chris J and Mnih, Andriy and Teh, Yee Whye and Kingdom, United and Kingdom, United},
	Booktitle = {International Conference of Learning Representations (ICLR)},
	File = {:Users/shanest/Documents/Library/Maddison et al/International Conference of Learning Representations (ICLR)/Maddison et al. - 2017 - The Concrete Distribution A Continuous Relaxation of Discrete Random Variables.pdf:pdf},
	Mendeley-Groups = {MachineLearning/emergent-communication},
	Title = {{The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables}},
	Url = {https://arxiv.org/pdf/1611.00712.pdf},
	Year = {2017},
	Bdsk-Url-1 = {https://arxiv.org/pdf/1611.00712.pdf}}

@article{Bengio2013,
	Abstract = {Stochastic neurons and hard non-linearities can be useful for a number of reasons in deep learning models, but in many cases they pose a challenging problem: how to estimate the gradient of a loss function with respect to the input of such stochastic or non-smooth neurons? I.e., can we "back-propagate" through these stochastic neurons? We examine this question, existing approaches, and compare four families of solutions, applicable in different settings. One of them is the minimum variance unbiased gradient estimator for stochatic binary neurons (a special case of the REINFORCE algorithm). A second approach, introduced here, decomposes the operation of a binary stochastic neuron into a stochastic binary part and a smooth differentiable part, which approximates the expected effect of the pure stochatic binary neuron to first order. A third approach involves the injection of additive or multiplicative noise in a computational graph that is otherwise differentiable. A fourth approach heuristically copies the gradient with respect to the stochastic output directly as an estimator of the gradient with respect to the sigmoid argument (we call this the straight-through estimator). To explore a context where these estimators are useful, we consider a small-scale version of {\em conditional computation}, where sparse stochastic units form a distributed representation of gaters that can turn off in combinatorially many ways large chunks of the computation performed in the rest of the neural network. In this case, it is important that the gating units produce an actual 0 most of the time. The resulting sparsity can be potentially be exploited to greatly reduce the computational cost of large deep networks for which conditional computation would be useful.},
	Archiveprefix = {arXiv},
	Arxivid = {1308.3432},
	Author = {Bengio, Yoshua and L{\'{e}}onard, Nicholas and Courville, Aaron},
	Eprint = {1308.3432},
	File = {:Users/shanest/Documents/Library/Bengio, L{\'{e}}onard, Courville/Unknown/Bengio, L{\'{e}}onard, Courville - 2013 - Estimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation.pdf:pdf},
	Mendeley-Groups = {MachineLearning/emergent-communication},
	Title = {{Estimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation}},
	Url = {http://arxiv.org/abs/1308.3432},
	Year = {2013},
	Bdsk-Url-1 = {http://arxiv.org/abs/1308.3432}}

@inproceedings{Jang2017,
	Author = {Jang, Eric and Gu, Shixiang and Poole, Ben},
	Booktitle = {International Conference of Learning Representations (ICLR)},
	File = {:Users/shanest/Documents/Library/Jang, Gu, Poole/International Conference of Learning Representations (ICLR)/Jang, Gu, Poole - 2017 - Categorical Reparameterization with Gumbel-Softmax.pdf:pdf},
	Mendeley-Groups = {MachineLearning},
	Title = {{Categorical Reparameterization with Gumbel-Softmax}},
	Url = {http://arxiv.org/abs/1611.01144},
	Year = {2017},
	Bdsk-Url-1 = {http://arxiv.org/abs/1611.01144}}
